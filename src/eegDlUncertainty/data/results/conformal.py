from abc import ABC, abstractmethod

import numpy as np


class Conformal(ABC):

    def __init__(self, alpha):
        self.alpha = alpha
        self.prediction_sets = None
        self.classes = ['Normal', 'MCI', 'Dementia']
        self._readable_sets = None

    @property
    def readable_sets(self):
        if self._readable_sets is not None:
            return self._readable_sets
        else:
            raise ValueError("Readable sets is None, make sure that calibrate and predict is called...")

    @abstractmethod
    def calibrate(self, cal_softmax_pred, cal_labels):
        pass

    @abstractmethod
    def predict(self, test_smx):
        pass

    def evaluate(self, test_labels):
        """
        Evaluates the coverage and average prediction set size of the prediction sets generated by the `predict` method against
        the actual test labels.

        Parameters
        ----------
        test_labels : np.ndarray
            An array of true labels for the test set, one-hot encoded, where each row corresponds
            to the one-hot encoded true class label for a corresponding sample.

        Returns
        -------
        dict
            A dictionary containing coverage and average prediction set size for each class.

        Raises
        ------
        ValueError
            If `prediction_sets` is None, indicating that the `predict` method has not been called
            prior to calling `evaluate`.

        Examples
        --------
        """
        if self.prediction_sets is None:
            raise ValueError("Prediction sets is None, make sure that predict is called before evaluate!")

        total_covered = {0: 0, 1: 0, 2: 0}  # Total times true label was included in the prediction sets
        total = {0: 0, 1: 0, 2: 0}  # Total samples per class
        total_set_size = {0: 0, 1: 0, 2: 0}  # Total size of prediction sets per class

        for i, label_vec in enumerate(test_labels):
            label_index = np.argmax(label_vec)
            # Count how many times the true label was included in the prediction sets
            if self.prediction_sets[i][label_index]:
                total_covered[label_index] += 1
            total[label_index] += 1
            # Add the size of the prediction set for this sample to the total for this class
            total_set_size[label_index] += np.sum(self.prediction_sets[i])

        # Calculate coverage and average prediction set size for each class
        results = {
            self.classes[label]: {
                'coverage': total_covered[label] / total[label],
                'average_set_size': total_set_size[label] / total[label]
            } for label in total
        }
        print(results)

        return results

    def transform_prediction_sets(self):
        """
        Transforms the boolean prediction sets into human-readable class labels.

        Returns
        -------
        list of lists
            A list where each element is a list of class labels included in the prediction set for each sample.

        Raises
        ------
        ValueError
            If `prediction_sets` is None, indicating that the `predict` method has not been called
            prior to calling this transformation.

        Examples
        --------
        # Assume `predict` has been called and `prediction_sets` populated
        # readable_sets = aps.transform_prediction_sets()
        """
        if self.prediction_sets is None:
            raise ValueError("Prediction sets is None. "
                             "Ensure that `predict` has been called before this transformation.")

        readable_sets = []
        for set_mask in self.prediction_sets:
            # Generate a list of class labels where the corresponding mask element is True
            current_set = [self.classes[idx] for idx, included in enumerate(set_mask) if included]
            readable_sets.append(current_set)

        self._readable_sets = readable_sets


class ConformalPredictionEqualWeighted(Conformal):

    def __init__(self, alpha):
        super().__init__(alpha=alpha)
        self.thresholds = []

    def calibrate(self, cal_softmax_pred, cal_labels):
        """
        Calibrate the confidence thresholds for conformal prediction using softmax probabilities
        and associated true labels. This function is specifically designed for a three-class
        prediction task.

        Parameters
        ----------
        cal_softmax_pred : np.ndarray
            An array of softmax probabilities with shape (n_samples, 3), where each row corresponds
            to the predicted probabilities for the three classes for a single sample.
        cal_labels : np.ndarray
            An array of true labels, one-hot encoded, with shape (n_samples, 3), where each row is
            the one-hot encoded true class label for a corresponding sample in `cal_softmax_pred`.

        Raises
        ------
        ValueError
            If the number of classes in `cal_softmax_pred` is not 3, as the calibration is
            specifically designed for three classes.

        Notes
        -----
        The method calculates the calibration score for each instance as 1 minus the predicted
        probability of the true class. It then uses these calibration scores to determine a
        threshold for each class such that a given proportion, defined by the adjusted alpha,
        of future predictions for that class will fall below the threshold, ensuring the desired
        confidence level.

        The thresholds are stored internally in the object after calibration to be used for
        making future prediction decisions with the specified confidence level.

        """
        if cal_softmax_pred.shape[1] != 3:
            raise ValueError("Conformal prediction is only available for multiclass atm...")
        # Initialize the needed values for calibration scores and number of instances in each class
        calibration_scores = {0: [], 1: [], 2: []}
        class_counts = {0: 0, 1: 0, 2: 0}

        for idx, prob in enumerate(cal_softmax_pred):
            # Get the label
            label_vec = cal_labels[idx]
            # Get the label index in the label vector
            label_index = np.argmax(label_vec)
            # Get the softmax score for the correct prediction
            correct_prob = prob[label_index]
            # Save the softmax score for the correct class -1
            calibration_scores[label_index].append(1 - correct_prob)
            # Count the class
            class_counts[label_index] += 1

        for class_label, scores in calibration_scores.items():
            # Check that the current class label has more than 0 instances
            if class_counts[class_label] > 0:
                # Apply a correction based on the class count
                correction = (class_counts[class_label] + 1) / class_counts[class_label]
                # Adjust the alpha based on the correction
                adjusted_alpha = self.alpha * correction
                # Find the threshold
                threshold = np.percentile(scores, 100 * (1 - adjusted_alpha))
                # Set the threshold
                self.thresholds.append(threshold)
            else:
                self.thresholds.append(0)

    def predict(self, test_softmax_pred):
        """
        Predicts the class labels for a set of test samples using calibrated thresholds.
        This method generates prediction sets for each sample where the actual class label
        is likely to fall, with confidence bounds defined by previously calculated thresholds.

        Parameters
        ----------
        test_softmax_pred : np.ndarray
            An array of softmax probabilities for the test set, where each row corresponds to
            the predicted probabilities for all classes for a single sample.

        Notes
        -----
        The method iterates through each softmax probability array of the test samples. For each
        class label, it computes the p-value as 1 minus the predicted probability of that class.
        It then checks if this p-value is less than the stored threshold for that class. If true,
        the class label is included in the prediction set for that sample, indicating that, with
        the specified confidence level, the true class label could be this class.

        After processing all samples, it stores the resulting prediction sets in the instance
        variable `prediction_sets`, which can be accessed for subsequent analysis or processing.
        """
        if not self.thresholds:
            raise ValueError("Thresholds not initialized")

        sets = []
        for prob in test_softmax_pred:
            temp_set = np.zeros_like(prob, dtype=bool)
            for class_label in range(len(self.thresholds)):
                p_value = 1 - prob[class_label]
                interval = p_value < self.thresholds[class_label]
                temp_set[class_label] = interval
            sets.append(temp_set)

        self.prediction_sets = sets
        self.transform_prediction_sets()


class AdaptivePredictionSets(Conformal):
    def __init__(self, alpha, lam_reg=0.01, kreg=1, disallow_zero_sets=False, rand=True):
        super().__init__(alpha=alpha)
        self.lam_reg = lam_reg
        self.k_reg = kreg
        self.disallow_zero_sets = disallow_zero_sets
        self.rand = rand
        self.thresholds = None

    def calibrate(self, cal_softmax_pred, cal_labels):
        n = cal_softmax_pred.shape[0]

        # Convert one-hot encoded labels to indices
        label_indexes = np.argmax(cal_labels, axis=1)

        # Sort indices of softmax predictions in descending order
        cal_pi = cal_softmax_pred.argsort(axis=1)[:, ::-1]

        # Find the index of each actual label in the sorted array
        cal_L = np.array([np.where(cal_pi[i] == label_indexes[i])[0][0] for i in range(n)])

        # Apply regularization if necessary
        if self.lam_reg > 0:
            reg_vec = np.array(self.k_reg * [0,] + (cal_softmax_pred.shape[1] - self.k_reg) * [self.lam_reg,])[None,:]
            cal_srt_reg = np.take_along_axis(cal_softmax_pred, cal_pi, axis=1) + reg_vec
        else:
            cal_srt_reg = np.take_along_axis(cal_softmax_pred, cal_pi, axis=1)

        # Calculate scores, incorporating randomness if enabled
        if self.rand:
            randomness = np.random.rand(n) * cal_srt_reg[np.arange(n), cal_L]
        else:
            randomness = 0

        cal_scores = cal_srt_reg.cumsum(axis=1)[np.arange(n), cal_L] - randomness

        # Calculate the quantile to set as the threshold
        self.thresholds = np.quantile(cal_scores, np.ceil((n + 1) * (1 - self.alpha)) / n, interpolation='higher')

    def predict(self, test_softmax_pred):
        """
        Predicts the class labels for a set of test samples using a calibrated threshold.
        This method generates prediction sets for each sample where the actual class label
        is likely to fall, with confidence bounds defined by the previously calculated threshold.

        Parameters
        ----------
        test_softmax_pred : np.ndarray
            An array of softmax probabilities for the test set, where each row corresponds to
            the predicted probabilities for all classes for a single sample.
        """
        if self.thresholds is None:
            raise ValueError("Threshold is None, call the function calibrate before calling predict!")

        prediction_sets = []
        for prob in test_softmax_pred:
            # Sort probabilities in descending order
            sorted_indices = np.argsort(prob)[::-1]
            sorted_prob = prob[sorted_indices]

            # Calculate cumulative probabilities
            cum_prob = np.cumsum(sorted_prob)

            # Include all classes where the cumulative probability up to that class is less than the threshold
            included_classes = sorted_indices[cum_prob <= self.thresholds]

            # Create a boolean array marking included classes
            set_mask = np.zeros_like(prob, dtype=bool)
            set_mask[included_classes] = True
            # Check for zero-sized sets and adjust if disallow_zero_sets is True
            if self.disallow_zero_sets and not set_mask.any():
                # Include the class with the highest probability
                set_mask[sorted_indices[0]] = True

            # Append to list of prediction sets
            prediction_sets.append(set_mask)

        self.prediction_sets = prediction_sets
        self.transform_prediction_sets()
